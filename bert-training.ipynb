{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import nltk\n\nnltk.download('stopwords')\nnltk.download('wordnet')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom tensorflow.keras import layers\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\nimport statistics\nfrom numpy import array\nfrom numpy import asarray\nfrom numpy import zeros\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras import layers\nimport tensorflow as tf\nfrom tensorflow import keras\n!pip install transformers\n!pip install contractions\n!pip install spacy==2.2.3\n!python -m spacy download en_core_web_sm\n!pip install beautifulsoup4==4.9.1\n!pip install textblob==0.15.3\n!pip install contractions\n!pip install transformers\n\nimport contractions\nimport re\nimport transformers\nfrom transformers import BertTokenizer, TFBertForSequenceClassification","metadata":{"id":"0529e0da","outputId":"66c00fa4-15d3-4e9f-eeae-cbb10b83b280","execution":{"iopub.status.busy":"2022-06-24T13:31:54.615684Z","iopub.execute_input":"2022-06-24T13:31:54.616151Z","iopub.status.idle":"2022-06-24T13:33:45.089786Z","shell.execute_reply.started":"2022-06-24T13:31:54.616112Z","shell.execute_reply":"2022-06-24T13:33:45.088886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ntrain = pd.read_csv(r'../input/emotions-dataset-for-nlp/train.txt', names=['sentences', 'emotion'], sep=';')\nval = pd.read_csv(r'../input/emotions-dataset-for-nlp/val.txt', names=['sentences', 'emotion'], sep=';')\ntest = pd.read_csv(r'../input/emotions-dataset-for-nlp/test.txt', names=['sentences', 'emotion'], sep=';')\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-24T13:31:33.362132Z","iopub.execute_input":"2022-06-24T13:31:33.362838Z","iopub.status.idle":"2022-06-24T13:31:33.464904Z","shell.execute_reply.started":"2022-06-24T13:31:33.362797Z","shell.execute_reply":"2022-06-24T13:31:33.464171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Lemmatizer_stop_word(sentence):\n    stop_words = set(stopwords.words('english'))\n    lemmatizer = WordNetLemmatizer() #look at other Lemmatizers and stemmers\n    sentence = re.sub('[^A-z]', ' ', sentence)\n    negative = ['not', 'neither', 'nor', 'but', 'however',\n                'although', 'nonetheless', 'despite', 'except',\n                        'even though', 'yet','unless']\n    stop_words = [z for z in stop_words if z not in negative]\n    preprocessed_tokens = [lemmatizer.lemmatize(contractions.fix(temp.lower())) for temp in sentence.split() if temp not in stop_words] #lemmatization\n    return ' '.join([x for x in preprocessed_tokens]).strip()","metadata":{"execution":{"iopub.status.busy":"2022-06-24T13:33:45.1026Z","iopub.execute_input":"2022-06-24T13:33:45.102985Z","iopub.status.idle":"2022-06-24T13:33:45.110902Z","shell.execute_reply.started":"2022-06-24T13:33:45.102947Z","shell.execute_reply":"2022-06-24T13:33:45.11019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nltk.download('omw-1.4')\ntrain['sentences'] = train['sentences'].apply(lambda x: Lemmatizer_stop_word(x))\nval['sentences'] = val['sentences'].apply(lambda x: Lemmatizer_stop_word(x))\ntest['sentences'] = test['sentences'].apply(lambda x: Lemmatizer_stop_word(x))\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-24T13:33:45.112834Z","iopub.execute_input":"2022-06-24T13:33:45.113417Z","iopub.status.idle":"2022-06-24T13:33:54.867036Z","shell.execute_reply.started":"2022-06-24T13:33:45.11338Z","shell.execute_reply":"2022-06-24T13:33:54.86615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_length=43\nfrom transformers import AutoTokenizer, TFBertModel\ntokenizer=AutoTokenizer.from_pretrained('bert-base-cased')\nbert=TFBertModel.from_pretrained('bert-base-cased')\nfrom tensorflow.keras.layers import Input, Dense\nx_train = tokenizer(\n    [x.split() for x in train['sentences']],\n    add_special_tokens=True,\n    max_length=max_length,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = True,\n    is_split_into_words=True,\n    verbose = True)\n\n\nx_val = tokenizer(\n    [x.split() for x in val['sentences']],\n    add_special_tokens=True,\n    max_length=max_length,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = True,\n    is_split_into_words=True,\n    verbose = True)","metadata":{"id":"PJFUKQrpfs0J","execution":{"iopub.status.busy":"2022-06-24T13:34:12.208913Z","iopub.execute_input":"2022-06-24T13:34:12.209274Z","iopub.status.idle":"2022-06-24T13:34:40.442363Z","shell.execute_reply.started":"2022-06-24T13:34:12.209243Z","shell.execute_reply":"2022-06-24T13:34:40.441347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(x_train)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T13:35:45.198678Z","iopub.execute_input":"2022-06-24T13:35:45.199057Z","iopub.status.idle":"2022-06-24T13:35:45.207247Z","shell.execute_reply.started":"2022-06-24T13:35:45.199027Z","shell.execute_reply":"2022-06-24T13:35:45.206312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lb = LabelEncoder()\nlabels_train=lb.fit(train.loc[:,'emotion'].to_list())\nprint(labels_train)\nlabels_train=lb.transform(train.loc[:,'emotion'].to_list())\nprint(labels_train)\nlabels_val=lb.transform(val.loc[:,'emotion'].to_list())\nlabels_test=lb.transform(test.loc[:,'emotion'].to_list())","metadata":{"execution":{"iopub.status.busy":"2022-06-24T13:36:11.139714Z","iopub.execute_input":"2022-06-24T13:36:11.140255Z","iopub.status.idle":"2022-06-24T13:36:11.175866Z","shell.execute_reply.started":"2022-06-24T13:36:11.14021Z","shell.execute_reply":"2022-06-24T13:36:11.175072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(labels_train[211])","metadata":{"execution":{"iopub.status.busy":"2022-06-24T13:44:18.60927Z","iopub.execute_input":"2022-06-24T13:44:18.609835Z","iopub.status.idle":"2022-06-24T13:44:18.62408Z","shell.execute_reply.started":"2022-06-24T13:44:18.609792Z","shell.execute_reply":"2022-06-24T13:44:18.623385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.random.set_seed(79)\n#max_len = 43\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.initializers import TruncatedNormal\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nfrom tensorflow.keras.metrics import CategoricalAccuracy\nfrom tensorflow.keras.utils import to_categorical\n\n\ninput_ids = Input(shape=(max_length,), dtype=tf.int32, name=\"input_ids\")\ninput_mask = Input(shape=(max_length,), dtype=tf.int32, name=\"attention_mask\")\n# embeddings = dbert_model(input_ids,attention_mask = input_mask)[0]\n\n\nembeddings = bert(input_ids,attention_mask = input_mask)[0] #(0 is the last hidden states,1 is the pooler_output)\nx = tf.keras.layers.GlobalMaxPool1D()(embeddings)\nx = Dense(138, activation='elu',kernel_initializer='GlorotNormal')(x)\nx = tf.keras.layers.Dropout(0.1)(x)\nx = Dense(28,activation = 'elu',kernel_initializer='GlorotNormal')(x)\n\noutput = Dense(6,activation = 'softmax')(x)\n    \nmodel = tf.keras.Model(inputs=[input_ids, input_mask], outputs=output)\nmodel.layers[2].trainable = True\n\n\nopt = Adam(\n    learning_rate=5e-05, # works well with BERTs\n    epsilon=1e-08,\n    decay=0.01,\n    clipnorm=1.0)\n\nmodel.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy']) \n\n#'sparse_categorical_crossentropy' for not one-hot encoded features\n# summarize the model\nprint(model.summary())\n\n# fit the model\nearly_stopping_cb=keras.callbacks.EarlyStopping(patience=2,restore_best_weights=True)\n#bert_model.trainable = False\n\nhistory = model.fit(\n    x ={'input_ids':x_train['input_ids'],'attention_mask':x_train['attention_mask']} ,\n    y =labels_train,\n    validation_data = (\n    {'input_ids':x_val['input_ids'],'attention_mask':x_val['attention_mask']}, labels_val\n    ),\n  epochs=3,\n    batch_size=12,callbacks=[early_stopping_cb]\n)\n","metadata":{"id":"8e3cdc07","outputId":"6497e6c1-ade0-48db-d741-9dc7607d9c3f","execution":{"iopub.status.busy":"2022-06-24T11:16:39.629321Z","iopub.execute_input":"2022-06-24T11:16:39.629948Z","iopub.status.idle":"2022-06-24T11:26:35.294105Z","shell.execute_reply.started":"2022-06-24T11:16:39.629901Z","shell.execute_reply":"2022-06-24T11:26:35.293007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\npd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\n#plt.gca().set_xlim(0,33)\nplt.gca().set_ylim(0,1)\nloss, accuracy = model.evaluate({'input_ids':x_val['input_ids'],'attention_mask':x_val['attention_mask']}, labels_val\n    )\nprint('Accuracy: %f' % (accuracy*100))","metadata":{"execution":{"iopub.status.busy":"2022-06-24T11:26:35.296115Z","iopub.execute_input":"2022-06-24T11:26:35.297116Z","iopub.status.idle":"2022-06-24T11:26:46.66934Z","shell.execute_reply.started":"2022-06-24T11:26:35.297069Z","shell.execute_reply":"2022-06-24T11:26:46.668243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n\ninput_ids = Input(shape=(max_length,), dtype=tf.int32, name=\"input_ids\")\ninput_mask = Input(shape=(max_length,), dtype=tf.int32, name=\"attention_mask\")\n\n\nembeddings = bert(input_ids,attention_mask = input_mask)[0] #(0 is the last hidden states,1 is the pooler_output)\nx = tf.keras.layers.GlobalMaxPool1D()(embeddings)\nx = Dense(138, activation='elu',kernel_initializer='GlorotNormal')(x)\nx = tf.keras.layers.Dropout(0.1)(x)\nx = Dense(28,activation = 'elu',kernel_initializer='GlorotNormal')(x)\n\noutput = Dense(6,activation = 'softmax')(x)\n    \nmodel_saved = tf.keras.Model(inputs=[input_ids, input_mask], outputs=output)\nmodel_saved.layers[2].trainable = True\n\n\nmodel_saved.load_weights('Bert_stepword_lemmatizer.h5')","metadata":{"execution":{"iopub.status.busy":"2022-06-24T11:26:46.671334Z","iopub.execute_input":"2022-06-24T11:26:46.671976Z","iopub.status.idle":"2022-06-24T11:26:48.957306Z","shell.execute_reply.started":"2022-06-24T11:26:46.671931Z","shell.execute_reply":"2022-06-24T11:26:48.956167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_saved.summary()\n\nfor i, layer in enumerate (model.layers):\n    print (i, layer)\n    try:\n        print (\"    \",layer.activation)\n    except AttributeError:\n        print('   no activation attribute')\n\nfor i in range(len(model.layers)):\n    print(f'{i}   {model.layers[i]}: \\n{model.layers[i].get_config()} \\n')\n#info about optimizers\nmodel.optimizer.get_config()        ","metadata":{"execution":{"iopub.status.busy":"2022-06-24T11:26:48.961048Z","iopub.execute_input":"2022-06-24T11:26:48.9621Z","iopub.status.idle":"2022-06-24T11:26:48.998224Z","shell.execute_reply.started":"2022-06-24T11:26:48.96205Z","shell.execute_reply":"2022-06-24T11:26:48.997132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss, accuracy = model.evaluate({'input_ids':x_test['input_ids'],'attention_mask':x_test['attention_mask']}, labels_test\n    )\nprint('Accuracy: %f' % (accuracy*100))","metadata":{"execution":{"iopub.status.busy":"2022-06-24T11:26:49.198767Z","iopub.execute_input":"2022-06-24T11:26:49.199257Z","iopub.status.idle":"2022-06-24T11:26:59.485052Z","shell.execute_reply.started":"2022-06-24T11:26:49.199214Z","shell.execute_reply":"2022-06-24T11:26:59.483852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.config.experimental.list_physical_devices('GPU')","metadata":{"execution":{"iopub.status.busy":"2022-06-24T11:26:59.486457Z","iopub.execute_input":"2022-06-24T11:26:59.487716Z","iopub.status.idle":"2022-06-24T11:26:59.49681Z","shell.execute_reply.started":"2022-06-24T11:26:59.487666Z","shell.execute_reply":"2022-06-24T11:26:59.495507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#class-label conversion\nfin_labels=[i.replace(\"\\n\", \"\") for i in train.loc[:,'emotion'].to_list()]\ndict(zip(labels_train,fin_labels))","metadata":{"execution":{"iopub.status.busy":"2022-06-24T11:26:59.498572Z","iopub.execute_input":"2022-06-24T11:26:59.499989Z","iopub.status.idle":"2022-06-24T11:26:59.519901Z","shell.execute_reply.started":"2022-06-24T11:26:59.499942Z","shell.execute_reply":"2022-06-24T11:26:59.518681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y=input()","metadata":{"execution":{"iopub.status.busy":"2022-06-24T11:26:59.521957Z","iopub.execute_input":"2022-06-24T11:26:59.522734Z","iopub.status.idle":"2022-06-24T11:27:19.432512Z","shell.execute_reply.started":"2022-06-24T11:26:59.522675Z","shell.execute_reply":"2022-06-24T11:27:19.431439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_s=pd.Series([y])\ny_lemm=y_s.apply(lambda x: Lemmatizer_stop_word(x))\ny_tok = tokenizer(\n    [x.split() for x in y_lemm],\n    add_special_tokens=True,\n    max_length=max_length,\n    truncation=True,\n    padding='max_length',  #only for sentence prediction \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = True,\n    is_split_into_words=True,\n    verbose = True)\n#labels_y=lb.transform(test.loc[:,'emotion'].to_list())\ny_prob=model.predict({'input_ids':y_tok['input_ids'],'attention_mask':y_tok['attention_mask']})*100\n#y_tok\nclass_label=y_prob.argmax(axis=-1)\nlb.inverse_transform(class_label) #from class to label","metadata":{"execution":{"iopub.status.busy":"2022-06-24T11:27:19.434004Z","iopub.execute_input":"2022-06-24T11:27:19.434818Z","iopub.status.idle":"2022-06-24T11:27:22.681302Z","shell.execute_reply.started":"2022-06-24T11:27:19.434759Z","shell.execute_reply":"2022-06-24T11:27:22.680075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"{'input_ids':y_tok['input_ids'],'attention_mask':y_tok['attention_mask']} #bert input parameters","metadata":{"execution":{"iopub.status.busy":"2022-06-24T11:27:22.683188Z","iopub.execute_input":"2022-06-24T11:27:22.683714Z","iopub.status.idle":"2022-06-24T11:27:22.696037Z","shell.execute_reply.started":"2022-06-24T11:27:22.683668Z","shell.execute_reply":"2022-06-24T11:27:22.694623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_saved.save(\"./bert_model.h5\")","metadata":{"execution":{"iopub.status.busy":"2022-06-24T11:27:57.209638Z","iopub.execute_input":"2022-06-24T11:27:57.210425Z","iopub.status.idle":"2022-06-24T11:27:58.108809Z","shell.execute_reply.started":"2022-06-24T11:27:57.210376Z","shell.execute_reply":"2022-06-24T11:27:58.107703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom IPython.display import FileLink\nFileLink(r'./bert_model.h5')","metadata":{"execution":{"iopub.status.busy":"2022-06-24T11:28:40.976998Z","iopub.execute_input":"2022-06-24T11:28:40.977452Z","iopub.status.idle":"2022-06-24T11:28:40.985049Z","shell.execute_reply.started":"2022-06-24T11:28:40.977421Z","shell.execute_reply":"2022-06-24T11:28:40.98401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}